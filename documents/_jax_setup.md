# JAX í™˜ê²½ êµ¬ì¶• ë§¤ë‰´ì–¼

**WSL2 + Anaconda + JAX + CUDA í™˜ê²½ êµ¬ì¶•í•˜ê¸°**

Googleì˜ ê³ ì„±ëŠ¥ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ JAX ì„¤ì¹˜ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ëª©ì°¨
1. [JAX ì†Œê°œ](#1-jax-ì†Œê°œ)
2. [í™˜ê²½ ìƒì„± ë° ì„¤ì¹˜](#2-í™˜ê²½-ìƒì„±-ë°-ì„¤ì¹˜)
3. [ì„¤ì¹˜ í™•ì¸ ë° í…ŒìŠ¤íŠ¸](#3-ì„¤ì¹˜-í™•ì¸-ë°-í…ŒìŠ¤íŠ¸)
4. [JAX ê¸°ë³¸ ì‚¬ìš©ë²•](#4-jax-ê¸°ë³¸-ì‚¬ìš©ë²•)
5. [ê³ ê¸‰ ê¸°ëŠ¥](#5-ê³ ê¸‰-ê¸°ëŠ¥)
6. [Flaxì™€ Optax ì„¤ì¹˜](#6-flaxì™€-optax-ì„¤ì¹˜)
7. [ë¬¸ì œ í•´ê²°](#7-ë¬¸ì œ-í•´ê²°)

---

## 1. JAX ì†Œê°œ

### JAXë€?
- **Google Research**ì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **NumPy API**ì™€ ê±°ì˜ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
- **ìë™ ë¯¸ë¶„(AutoGrad)** ê¸°ëŠ¥ ë‚´ì¥
- **JIT ì»´íŒŒì¼(XLA)** ì§€ì›ìœ¼ë¡œ ë§¤ìš° ë¹ ë¥¸ ì†ë„
- **ìë™ ë²¡í„°í™”(vmap)** ë° ë³‘ë ¬í™”(pmap)

### ì£¼ìš” íŠ¹ì§•
- **NumPy í˜¸í™˜**: `import jax.numpy as jnp`ë¡œ NumPy ëŒ€ì²´
- **í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°**: Pure function ê¸°ë°˜
- **Composable transformations**: `grad`, `jit`, `vmap`, `pmap`
- **GPU/TPU ì§€ì›**: ìë™ ê°€ì†

### ì‚¬ìš© ì‚¬ë¡€
- ë”¥ëŸ¬ë‹ ì—°êµ¬ (Flax, Haiku, Equinox)
- ê³¼í•™ ê³„ì‚° ë° ì‹œë®¬ë ˆì´ì…˜
- ê°•í™”í•™ìŠµ (RLax, Acme)
- í™•ë¥ ë¡ ì  í”„ë¡œê·¸ë˜ë° (NumPyro)

---

## 2. í™˜ê²½ ìƒì„± ë° ì„¤ì¹˜

### 2-1. JAX í™˜ê²½ ìƒì„±

```bash
# í™˜ê²½ ë¹„í™œì„±í™”
conda deactivate

# Python 3.10ìœ¼ë¡œ í™˜ê²½ ìƒì„±
conda create -n jax_env python=3.10 -y

# í™˜ê²½ í™œì„±í™”
conda activate jax_env
```

### 2-2. JAX ì„¤ì¹˜ (CUDA ì§€ì›)

JAXëŠ” **CUDA ë²„ì „ì— ë”°ë¼** ë‹¤ë¥¸ ì„¤ì¹˜ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### CUDA ë²„ì „ í™•ì¸

```bash
nvidia-smi
# ì¶œë ¥ì—ì„œ "CUDA Version: 12.x" í™•ì¸
```

#### CUDA 12.xìš© JAX ì„¤ì¹˜ (ê¶Œì¥)

```bash
# JAX with CUDA 12.x support
pip install --upgrade "jax[cuda12]"
```

#### CUDA 11.xìš© JAX ì„¤ì¹˜

```bash
# JAX with CUDA 11.x support (ë ˆê±°ì‹œ)
pip install --upgrade "jax[cuda11_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

#### CPU ì „ìš© ì„¤ì¹˜ (í…ŒìŠ¤íŠ¸ìš©)

```bash
# CPU only
pip install --upgrade jax
```

### 2-3. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# NumPy ë° SciPy
conda install -y numpy scipy

# ì‹œê°í™”
pip install matplotlib seaborn plotly

# ë°ì´í„° ì²˜ë¦¬
conda install -y pandas

# ìœ í‹¸ë¦¬í‹°
pip install tqdm rich

# Jupyter
conda install -y jupyter ipykernel
```

### 2-4. JAX ìƒíƒœê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# Flax (ì‹ ê²½ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬)
pip install flax

# Optax (ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬)
pip install optax

# Orbax (ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬)
pip install orbax-checkpoint

# Chex (í…ŒìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹°)
pip install chex

# Equinox (í•¨ìˆ˜í˜• ì‹ ê²½ë§)
pip install equinox

# JAXtyping (íƒ€ì… íŒíŠ¸)
pip install jaxtyping
```

---

## 3. ì„¤ì¹˜ í™•ì¸ ë° í…ŒìŠ¤íŠ¸

### 3-1. ê¸°ë³¸ Import í…ŒìŠ¤íŠ¸

```bash
python << EOF
import sys
print("="*60)
print("JAX Installation Check")
print("="*60)

# 1. JAX ë²„ì „
import jax
print(f"JAX version: {jax.__version__}")

# 2. ë°±ì—”ë“œ í™•ì¸
import jax.numpy as jnp
print(f"Default backend: {jax.default_backend()}")

# 3. ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤
devices = jax.devices()
print(f"\nAvailable devices: {len(devices)}")
for i, device in enumerate(devices):
    print(f"  Device {i}: {device}")

# 4. GPU ì •ë³´
if jax.default_backend() == 'gpu':
    from jax.lib import xla_bridge
    print(f"\nGPU backend: {xla_bridge.get_backend().platform}")
    print(f"GPU count: {jax.device_count()}")

print("\nâœ“ JAX installation successful!")
print("="*60)
EOF
```

**ì˜ˆìƒ ì¶œë ¥**:
```
============================================================
JAX Installation Check
============================================================
JAX version: 0.4.35
Default backend: gpu

Available devices: 1
  Device 0: cuda:0

GPU backend: CUDA
GPU count: 1

âœ“ JAX installation successful!
============================================================
```

### 3-2. GPU ì—°ì‚° í…ŒìŠ¤íŠ¸

```bash
python << EOF
import jax
import jax.numpy as jnp
import time

print("\n=== GPU Computation Test ===\n")

# GPUì—ì„œ í–‰ë ¬ ìƒì„±
key = jax.random.PRNGKey(0)
x = jax.random.normal(key, (5000, 5000))
y = jax.random.normal(key, (5000, 5000))

# JIT ì»´íŒŒì¼ëœ í–‰ë ¬ ê³±ì…ˆ
@jax.jit
def matmul(a, b):
    return jnp.dot(a, b)

# ì²« ì‹¤í–‰ (ì»´íŒŒì¼ í¬í•¨)
print("First run (with compilation)...")
start = time.time()
z = matmul(x, y).block_until_ready()
first_time = time.time() - start
print(f"Time: {first_time:.4f} seconds")

# ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì»´íŒŒì¼ ì œì™¸)
print("\nSecond run (cached)...")
start = time.time()
z = matmul(x, y).block_until_ready()
second_time = time.time() - start
print(f"Time: {second_time:.4f} seconds")

print(f"\nResult shape: {z.shape}")
print(f"Speedup: {first_time/second_time:.2f}x")
print("\nâœ“ GPU computation successful!")
EOF
```

### 3-3. NumPy vs JAX ì†ë„ ë¹„êµ

```bash
python << EOF
import numpy as np
import jax.numpy as jnp
import jax
import time

print("\n=== NumPy vs JAX Speed Comparison ===\n")

size = 3000

# NumPy (CPU)
print("NumPy (CPU) computing...")
x_np = np.random.randn(size, size).astype(np.float32)
y_np = np.random.randn(size, size).astype(np.float32)
start = time.time()
z_np = np.matmul(x_np, y_np)
cpu_time = time.time() - start

# JAX (GPU) - JIT ì»´íŒŒì¼
@jax.jit
def jax_matmul(a, b):
    return jnp.matmul(a, b)

print("JAX (GPU) computing...")
key = jax.random.PRNGKey(0)
x_jax = jax.random.normal(key, (size, size), dtype=jnp.float32)
y_jax = jax.random.normal(key, (size, size), dtype=jnp.float32)

# Warm-up
_ = jax_matmul(x_jax, y_jax).block_until_ready()

start = time.time()
z_jax = jax_matmul(x_jax, y_jax).block_until_ready()
gpu_time = time.time() - start

print(f"\n{'='*50}")
print(f"Matrix size: {size}x{size}")
print(f"CPU (NumPy) time: {cpu_time:.4f} seconds")
print(f"GPU (JAX) time: {gpu_time:.4f} seconds")
print(f"Speedup: {cpu_time/gpu_time:.2f}x faster")
print(f"{'='*50}")
EOF
```

---

## 4. JAX ê¸°ë³¸ ì‚¬ìš©ë²•

### 4-1. NumPy ìŠ¤íƒ€ì¼ ë°°ì—´ ì—°ì‚°

```bash
python << 'EOF'
import jax.numpy as jnp
import jax

print("\n=== JAX Basic Operations ===\n")

# ë°°ì—´ ìƒì„±
x = jnp.array([1, 2, 3, 4, 5])
print(f"Array: {x}")
print(f"Type: {type(x)}")
print(f"Device: {x.device()}")

# ìˆ˜í•™ ì—°ì‚°
y = jnp.exp(x)
z = jnp.sqrt(x)
print(f"\nexp(x): {y}")
print(f"sqrt(x): {z}")

# í–‰ë ¬ ì—°ì‚°
A = jnp.array([[1, 2], [3, 4]])
B = jnp.array([[5, 6], [7, 8]])
C = jnp.dot(A, B)
print(f"\nMatrix multiplication:\n{C}")

# ë‚œìˆ˜ ìƒì„± (PRNG key í•„ìš”)
key = jax.random.PRNGKey(42)
random_array = jax.random.normal(key, (3, 3))
print(f"\nRandom array:\n{random_array}")
EOF
```

### 4-2. ìë™ ë¯¸ë¶„ (grad)

```bash
python << 'EOF'
import jax
import jax.numpy as jnp

print("\n=== Automatic Differentiation ===\n")

# í•¨ìˆ˜ ì •ì˜
def f(x):
    return x**3 + 2*x**2 - 5*x + 3

# ë¯¸ë¶„ í•¨ìˆ˜ ìƒì„±
df = jax.grad(f)

# ê³„ì‚°
x = 2.0
y = f(x)
dy = df(x)

print(f"f({x}) = {y}")
print(f"f'({x}) = {dy}")

# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜
def g(x, y):
    return x**2 + y**2

# í¸ë¯¸ë¶„
dg_dx = jax.grad(g, argnums=0)
dg_dy = jax.grad(g, argnums=1)

x, y = 3.0, 4.0
print(f"\ng({x}, {y}) = {g(x, y)}")
print(f"âˆ‚g/âˆ‚x = {dg_dx(x, y)}")
print(f"âˆ‚g/âˆ‚y = {dg_dy(x, y)}")
EOF
```

### 4-3. JIT ì»´íŒŒì¼

```bash
python << 'EOF'
import jax
import jax.numpy as jnp
import time

print("\n=== JIT Compilation ===\n")

# ì¼ë°˜ í•¨ìˆ˜
def slow_function(x):
    return jnp.sum(x ** 2) + jnp.sum(x ** 3)

# JIT ì»´íŒŒì¼ëœ í•¨ìˆ˜
@jax.jit
def fast_function(x):
    return jnp.sum(x ** 2) + jnp.sum(x ** 3)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
x = jax.random.normal(jax.random.PRNGKey(0), (10000,))

# ì†ë„ ë¹„êµ
print("Without JIT:")
start = time.time()
for _ in range(100):
    result = slow_function(x).block_until_ready()
no_jit_time = time.time() - start
print(f"Time: {no_jit_time:.4f} seconds")

print("\nWith JIT:")
# Warm-up
_ = fast_function(x).block_until_ready()
start = time.time()
for _ in range(100):
    result = fast_function(x).block_until_ready()
jit_time = time.time() - start
print(f"Time: {jit_time:.4f} seconds")

print(f"\nSpeedup: {no_jit_time/jit_time:.2f}x")
EOF
```

### 4-4. ë²¡í„°í™” (vmap)

```bash
python << 'EOF'
import jax
import jax.numpy as jnp

print("\n=== Vectorization with vmap ===\n")

# ë‹¨ì¼ ì…ë ¥ í•¨ìˆ˜
def single_prediction(params, x):
    return jnp.dot(params, x)

# ë°°ì¹˜ ì²˜ë¦¬ (ìˆ˜ë™)
def batch_prediction_manual(params, X):
    return jnp.array([single_prediction(params, x) for x in X])

# ë°°ì¹˜ ì²˜ë¦¬ (vmap)
batch_prediction_vmap = jax.vmap(single_prediction, in_axes=(None, 0))

# í…ŒìŠ¤íŠ¸
params = jnp.array([1.0, 2.0, 3.0])
X = jax.random.normal(jax.random.PRNGKey(0), (5, 3))

result_manual = batch_prediction_manual(params, X)
result_vmap = batch_prediction_vmap(params, X)

print(f"Manual result: {result_manual}")
print(f"vmap result: {result_vmap}")
print(f"Results match: {jnp.allclose(result_manual, result_vmap)}")
EOF
```

---

## 5. ê³ ê¸‰ ê¸°ëŠ¥

### 5-1. ë‹¤ì¤‘ GPU ë³‘ë ¬í™” (pmap)

```bash
python << 'EOF'
import jax
import jax.numpy as jnp

print("\n=== Parallel Computation (pmap) ===\n")

# GPU ê°œìˆ˜ í™•ì¸
n_devices = jax.device_count()
print(f"Available devices: {n_devices}")

if n_devices > 1:
    # ë³‘ë ¬ í•¨ìˆ˜ ì •ì˜
    @jax.pmap
    def parallel_square(x):
        return x ** 2
    
    # ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ë¡œ ë¶„í• 
    x = jnp.arange(n_devices * 4).reshape(n_devices, 4)
    print(f"Input: {x}")
    
    # ë³‘ë ¬ ì‹¤í–‰
    result = parallel_square(x)
    print(f"Result: {result}")
else:
    print("Only 1 GPU available, pmap example skipped")
    print("pmap is useful when you have multiple GPUs")
EOF
```

### 5-2. Pytree ì‚¬ìš©

```bash
python << 'EOF'
import jax
import jax.numpy as jnp

print("\n=== PyTree Operations ===\n")

# Pytree: ì¤‘ì²©ëœ íŒŒì´ì¬ êµ¬ì¡°
params = {
    'layer1': {'w': jnp.ones((3, 4)), 'b': jnp.zeros(4)},
    'layer2': {'w': jnp.ones((4, 2)), 'b': jnp.zeros(2)},
}

# Pytree map
def scale_params(params, factor):
    return jax.tree_map(lambda x: x * factor, params)

scaled = scale_params(params, 2.0)
print("Original layer1 w:")
print(params['layer1']['w'])
print("\nScaled layer1 w:")
print(scaled['layer1']['w'])

# Pytree flatten/unflatten
leaves, treedef = jax.tree_flatten(params)
print(f"\nNumber of parameters: {len(leaves)}")
print(f"Total elements: {sum(x.size for x in leaves)}")
EOF
```

### 5-3. ì»¤ìŠ¤í…€ Gradient

```bash
python << 'EOF'
import jax
import jax.numpy as jnp

print("\n=== Custom Gradient ===\n")

# ì»¤ìŠ¤í…€ ë¯¸ë¶„ ì •ì˜
@jax.custom_vjp
def f(x):
    return jnp.sin(x)

def f_fwd(x):
    return f(x), x

def f_bwd(x, g):
    # ì»¤ìŠ¤í…€ gradient: cos(x) ëŒ€ì‹  1.0 ì‚¬ìš©
    return (g * 1.0,)

f.defvjp(f_fwd, f_bwd)

# í…ŒìŠ¤íŠ¸
x = 1.0
y = f(x)
dy = jax.grad(f)(x)

print(f"f({x}) = {y}")
print(f"Custom gradient: {dy}")
print(f"True gradient (cos): {jnp.cos(x)}")
EOF
```

---

## 6. Flaxì™€ Optax ì„¤ì¹˜

### 6-1. Flax (ì‹ ê²½ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬)

```bash
python << 'EOF'
from flax import linen as nn
import jax
import jax.numpy as jnp

print("\n=== Flax Neural Network ===\n")

# ê°„ë‹¨í•œ MLP ì •ì˜
class MLP(nn.Module):
    features: tuple = (128, 64, 10)
    
    @nn.compact
    def __call__(self, x):
        for feat in self.features[:-1]:
            x = nn.Dense(feat)(x)
            x = nn.relu(x)
        x = nn.Dense(self.features[-1])(x)
        return x

# ëª¨ë¸ ì´ˆê¸°í™”
model = MLP()
key = jax.random.PRNGKey(0)
x = jax.random.normal(key, (32, 784))  # batch of 32

# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
params = model.init(key, x)

# Forward pass
output = model.apply(params, x)
print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Number of parameters: {sum(x.size for x in jax.tree_leaves(params))}")
print("\nâœ“ Flax model created successfully!")
EOF
```

### 6-2. Optax (ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬)

```bash
python << 'EOF'
import jax
import jax.numpy as jnp
import optax

print("\n=== Optax Optimizer ===\n")

# ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
def loss_fn(params, x, y):
    pred = jnp.dot(x, params)
    return jnp.mean((pred - y) ** 2)

# ìµœì í™” ì„¤ì •
learning_rate = 0.01
optimizer = optax.adam(learning_rate)

# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
key = jax.random.PRNGKey(0)
params = jax.random.normal(key, (10,))
opt_state = optimizer.init(params)

# ë”ë¯¸ ë°ì´í„°
x = jax.random.normal(key, (100, 10))
y = jax.random.normal(key, (100,))

# í•™ìŠµ ìŠ¤í… í•¨ìˆ˜
@jax.jit
def train_step(params, opt_state, x, y):
    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

# ëª‡ ë²ˆì˜ í•™ìŠµ ìŠ¤í…
print("Training...")
for i in range(5):
    params, opt_state, loss = train_step(params, opt_state, x, y)
    print(f"Step {i+1}, Loss: {loss:.6f}")

print("\nâœ“ Optax optimizer working!")
EOF
```

### 6-3. Flax + Optax í†µí•© ì˜ˆì‹œ

```bash
cat > ~/flax_training_example.py << 'EOF'
"""Flax + Optax í•™ìŠµ ì˜ˆì‹œ"""
import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.training import train_state
import optax
from tqdm import tqdm


class SimpleCNN(nn.Module):
    """ê°„ë‹¨í•œ CNN ëª¨ë¸"""
    
    @nn.compact
    def __call__(self, x, train: bool = True):
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        
        x = nn.Conv(features=64, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=128)(x)
        x = nn.relu(x)
        x = nn.Dense(features=10)(x)
        return x


def create_train_state(rng, learning_rate):
    """í•™ìŠµ ìƒíƒœ ìƒì„±"""
    model = SimpleCNN()
    params = model.init(rng, jnp.ones([1, 28, 28, 1]))['params']
    tx = optax.adam(learning_rate)
    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=params,
        tx=tx
    )


@jax.jit
def train_step(state, batch):
    """í•™ìŠµ ìŠ¤í…"""
    def loss_fn(params):
        logits = state.apply_fn({'params': params}, batch['image'])
        loss = optax.softmax_cross_entropy_with_integer_labels(
            logits=logits, labels=batch['label']
        ).mean()
        return loss
    
    loss, grads = jax.value_and_grad(loss_fn)(state.params)
    state = state.apply_gradients(grads=grads)
    return state, loss


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n=== Flax + Optax Training Example ===\n")
    
    # ì´ˆê¸°í™”
    rng = jax.random.PRNGKey(0)
    state = create_train_state(rng, learning_rate=1e-3)
    
    # ë”ë¯¸ ë°ì´í„°
    key = jax.random.PRNGKey(0)
    dummy_batch = {
        'image': jax.random.normal(key, (32, 28, 28, 1)),
        'label': jax.random.randint(key, (32,), 0, 10)
    }
    
    # í•™ìŠµ
    print("Training for 10 steps...")
    for step in tqdm(range(10)):
        state, loss = train_step(state, dummy_batch)
        if step % 2 == 0:
            print(f"Step {step}, Loss: {loss:.4f}")
    
    print("\nâœ“ Training completed successfully!")


if __name__ == "__main__":
    main()
EOF

# ì‹¤í–‰
python ~/flax_training_example.py
```

---

## 7. ë¬¸ì œ í•´ê²°

### 7-1. `No GPU/TPU found` ì—ëŸ¬

**ì›ì¸**: CUDAê°€ ì œëŒ€ë¡œ ì¸ì‹ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```bash
# JAX ì¬ì„¤ì¹˜
pip uninstall jax jaxlib -y
pip install --upgrade "jax[cuda12]"

# CUDA í™•ì¸
nvidia-smi

# WSL ì¬ì‹œì‘
# PowerShellì—ì„œ: wsl --shutdown
```

### 7-2. `XlaRuntimeError` ë°œìƒ

**ì›ì¸**: JIT ì»´íŒŒì¼ ì‹¤íŒ¨

**í•´ê²°**:
```python
# JIT ë¹„í™œì„±í™”í•˜ì—¬ í…ŒìŠ¤íŠ¸
jax.config.update('jax_disable_jit', True)

# ë˜ëŠ” ë””ë²„ê·¸ ëª¨ë“œ
jax.config.update('jax_debug_nans', True)
```

### 7-3. ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²°**:
```python
# GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹ ë¹„í™œì„±í™”
import os
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.7'  # 70%ë§Œ ì‚¬ìš©

import jax
```

### 7-4. Float64 ì •ë°€ë„ í™œì„±í™”

**ê¸°ë³¸ì ìœ¼ë¡œ JAXëŠ” Float32 ì‚¬ìš©**:
```python
import jax
jax.config.update("jax_enable_x64", True)

# í™•ì¸
import jax.numpy as jnp
x = jnp.array([1.0])
print(x.dtype)  # float64
```

### 7-5. CuDNN ê´€ë ¨ ì—ëŸ¬

**í•´ê²°**:
```bash
conda activate jax_env
conda install -c conda-forge cudnn -y
```

---

## 8. JAX vs PyTorch/TensorFlow ë¹„êµ

### 8-1. ì£¼ìš” ì°¨ì´ì 

| íŠ¹ì§• | JAX | PyTorch | TensorFlow |
|------|-----|---------|-----------|
| íŒ¨ëŸ¬ë‹¤ì„ | í•¨ìˆ˜í˜• | ê°ì²´ì§€í–¥ | í˜¼í•© |
| ìë™ ë¯¸ë¶„ | grad | autograd | GradientTape |
| ê°€ì† | JIT (XLA) | TorchScript | XLA |
| NumPy í˜¸í™˜ | ê±°ì˜ ì™„ë²½ | ìœ ì‚¬ | ë¶€ë¶„ì  |
| ê°€ë³€ì„± | Immutable | Mutable | Mutable |
| í•™ìŠµ ê³¡ì„  | ì¤‘ê°„ | ì‰¬ì›€ | ì–´ë ¤ì›€ |

### 8-2. ì½”ë“œ ë¹„êµ

**PyTorch**:
```python
import torch

x = torch.randn(100, 100, requires_grad=True)
y = x ** 2
loss = y.sum()
loss.backward()
grad = x.grad
```

**JAX**:
```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sum(x ** 2)

x = jax.random.normal(jax.random.PRNGKey(0), (100, 100))
grad_fn = jax.grad(f)
grad = grad_fn(x)
```

---

## 9. í™˜ê²½ ê´€ë¦¬

### 9-1. ë‹¨ì¶• ëª…ë ¹ ì¶”ê°€

```bash
cat >> ~/.bashrc << 'EOF'

# JAX í™˜ê²½ ë‹¨ì¶• ëª…ë ¹
alias jx='conda activate jax_env'
EOF

source ~/.bashrc
```

**ì‚¬ìš©ë²•**: `jx` â†’ jax_env í™œì„±í™”

### 9-2. íŒ¨í‚¤ì§€ ë°±ì—…

```bash
conda activate jax_env
conda list --export > ~/jax_env_packages.txt
pip freeze > ~/jax_requirements.txt
```

### 9-3. JAX ì„¤ì • íŒŒì¼

```bash
cat > ~/.jaxrc << 'EOF'
# JAX ê¸°ë³¸ ì„¤ì •
import os

# GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹ ë¹„í™œì„±í™”
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'

# Float64 í™œì„±í™”
os.environ['JAX_ENABLE_X64'] = '1'

# ë””ë²„ê·¸ ëª¨ë“œ (í•„ìš”ì‹œ)
# os.environ['JAX_DEBUG_NANS'] = '1'
EOF
```

Pythonì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°:
```python
import os
exec(open(os.path.expanduser('~/.jaxrc')).read())
import jax
```

---

## 10. ì „ì²´ ì„¤ì¹˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

| ë‹¨ê³„ | ë‚´ìš© | í™•ì¸ |
|------|------|------|
| 1 | jax_env í™˜ê²½ ìƒì„± | â˜ |
| 2 | JAX ì„¤ì¹˜ (CUDA 12.x) | â˜ |
| 3 | `import jax` ì„±ê³µ | â˜ |
| 4 | GPU ì¸ì‹ í™•ì¸ | â˜ |
| 5 | GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ | â˜ |
| 6 | JIT ì»´íŒŒì¼ í…ŒìŠ¤íŠ¸ | â˜ |
| 7 | ìë™ ë¯¸ë¶„ í…ŒìŠ¤íŠ¸ | â˜ |
| 8 | Flax ì„¤ì¹˜ | â˜ |
| 9 | Optax ì„¤ì¹˜ | â˜ |
| 10 | í†µí•© í•™ìŠµ ì˜ˆì‹œ | â˜ |

---

## 11. ì „ì²´ í™˜ê²½ ìš”ì•½

ì´ì œ **5ê°œì˜ ë…ë¦½ëœ ë”¥ëŸ¬ë‹ í™˜ê²½**ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!

| í™˜ê²½ | Python | ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ | ìš©ë„ |
|------|--------|----------------|------|
| pytorch_env | 3.10 | PyTorch 2.5.1 | ë²”ìš© ë”¥ëŸ¬ë‹ |
| cupy_env | 3.10 | CuPy 13.6.0 | NumPy GPU ê°€ì† |
| tensorflow_env | 3.10 | TensorFlow 2.18.0 | TensorFlow ë”¥ëŸ¬ë‹ |
| anomalib_env | 3.10 | Anomalib 1.1.x | ì´ìƒ ê°ì§€ |
| jax_env | 3.10 | JAX 0.4.x + Flax | í•¨ìˆ˜í˜• ë”¥ëŸ¬ë‹ |

### ë¹ ë¥¸ í™˜ê²½ ì „í™˜

```bash
pt   # PyTorch
cu   # CuPy
tf   # TensorFlow
al   # Anomalib
jx   # JAX
ca   # ë¹„í™œì„±í™”
gpu  # GPU ëª¨ë‹ˆí„°ë§
```

---

## 12. ì°¸ê³  ìë£Œ

- **JAX ê³µì‹ ë¬¸ì„œ**: https://jax.readthedocs.io/
- **JAX GitHub**: https://github.com/google/jax
- **Flax ë¬¸ì„œ**: https://flax.readthedocs.io/
- **Optax ë¬¸ì„œ**: https://optax.readthedocs.io/
- **JAX Ecosystem**: https://github.com/n2cholas/awesome-jax

---

**ì´ ë§¤ë‰´ì–¼ì„ ë”°ë¼í•˜ë©´ JAX í™˜ê²½ì„ ì™„ë²½í•˜ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸ‰