{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73a1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "COMMON_DIR = \"/mnt/d/github/computer_vision/books/generative_deep_learning/pytorch/common\"\n",
    "if COMMON_DIR not in sys.path:\n",
    "    sys.path.append(COMMON_DIR)\n",
    "\n",
    "from common.datasets import get_train_loader, get_test_loader\n",
    "from common.utils import set_seed, plot_images\n",
    "from common.trainer import fit, evaluate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795a45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d6f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train dataset: 50000, dataloader: 390\n",
      "train images: torch.Size([128, 3, 32, 32]), torch.float32, -1.0, 1.0\n",
      "train labels: torch.Size([128]), torch.int64, 0, 9\n",
      "\n",
      "test dataset: 10000, dataloader: 157\n",
      "test  images: torch.Size([64, 3, 32, 32]), torch.float32, -1.0, 1.0\n",
      "test  labels: torch.Size([64]), torch.int64, 0, 9\n"
     ]
    }
   ],
   "source": [
    "## Data Loading\n",
    "from common.datasets import CIFAR10\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "root_dir = \"/mnt/d/datasets/cifar10\"\n",
    "train_loader = get_train_loader(dataset=CIFAR10(root_dir, \"train\", transform=transform), batch_size=128)\n",
    "test_loader = get_test_loader(dataset=CIFAR10(root_dir, \"test\", transform=transform), batch_size=64)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntrain dataset: {len(train_loader.dataset)}, dataloader: {len(train_loader)}\")\n",
    "print(f\"train images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"train labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntest dataset: {len(test_loader.dataset)}, dataloader: {len(test_loader)}\")\n",
    "print(f\"test  images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"test  labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb58290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd50c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512, momentum=0.9),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv5 = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.view(-1, z.size(1), 1, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pred_step(self, z):\n",
    "        images = self.forward(z)\n",
    "        return dict(image=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e77a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP(nn.Module):\n",
    "    def __init__(self, critic, generator, latent_dim, critic_steps=5, gp_weight=10.0, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.critic = critic.to(self.device)\n",
    "        self.generator = generator.to(self.device)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.critic_steps = critic_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "        self.g_optimizer = None\n",
    "        self.c_optimizer = None\n",
    "\n",
    "    def gradient_penalty(self, real_images, fake_images, batch_size):\n",
    "        alpha = torch.randn(batch_size, 1, 1, 1).to(real_images.device)\n",
    "        interpolated = real_images * alpha + fake_images * (1 - alpha)\n",
    "        interpolated.requires_grad_(True)\n",
    "\n",
    "        pred = self.critic(interpolated)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=pred, inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(pred),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        norm = gradients.norm(2, dim=1)\n",
    "        gp = ((norm - 1.0) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        real_images = batch[\"image\"].to(self.device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Critic 학습 (여러 스텝)\n",
    "        for _ in range(self.critic_steps):\n",
    "            self.c_optimizer.zero_grad()\n",
    "            z = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "\n",
    "            fake_images = self.generator(z).detach()\n",
    "            real_preds = self.critic(real_images)\n",
    "            fake_preds = self.critic(fake_images)\n",
    "\n",
    "            c_wass_loss = fake_preds.mean() - real_preds.mean()\n",
    "            gp = self.gradient_penalty(real_images, fake_images, batch_size)\n",
    "            c_loss = c_wass_loss + gp * self.gp_weight\n",
    "\n",
    "            c_loss.backward()\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "        # Generator 학습\n",
    "        self.g_optimizer.zero_grad()\n",
    "        z = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "        fake_images = self.generator(z)\n",
    "        fake_preds = self.critic(fake_images)\n",
    "        g_loss = -fake_preds.mean()\n",
    "\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return dict(\n",
    "            c_loss=c_loss,\n",
    "            c_wass_loss=c_wass_loss,\n",
    "            c_gp=gp,\n",
    "            g_loss=g_loss,\n",
    "            real_score=real_preds.mean(),\n",
    "            fake_score=fake_preds.mean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304303b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
