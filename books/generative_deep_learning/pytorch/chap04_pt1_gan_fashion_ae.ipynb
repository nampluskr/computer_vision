{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd17d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "COMMON_DIR = \"/mnt/d/github/computer_vision/books/generative_deep_learning/pytorch/common\"\n",
    "if COMMON_DIR not in sys.path:\n",
    "    sys.path.append(COMMON_DIR)\n",
    "\n",
    "from common.datasets import get_train_loader, get_test_loader\n",
    "from common.utils import set_seed, plot_images, create_images\n",
    "from common.trainer import fit, evaluate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a77c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b321f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train dataset: 60000, dataloader: 600\n",
      "train images: torch.Size([100, 1, 32, 32]), torch.float32, 0.0, 1.0\n",
      "train labels: torch.Size([100]), torch.int64, 0, 9\n",
      "\n",
      "test dataset: 10000, dataloader: 157\n",
      "test  images: torch.Size([64, 1, 32, 32]), torch.float32, 0.0, 1.0\n",
      "test  labels: torch.Size([64]), torch.int64, 0, 9\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "from common.datasets import FashionMNIST, get_train_loader, get_test_loader\n",
    "\n",
    "root_dir = \"/mnt/d/datasets/fashion_mnist\"\n",
    "train_loader = get_train_loader(dataset=FashionMNIST(root_dir, \"train\"), batch_size=100)\n",
    "test_loader = get_test_loader(dataset=FashionMNIST(root_dir, \"test\"), batch_size=64)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntrain dataset: {len(train_loader.dataset)}, dataloader: {len(train_loader)}\")\n",
    "print(f\"train images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"train labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntest dataset: {len(test_loader.dataset)}, dataloader: {len(test_loader)}\")\n",
    "print(f\"test  images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"test  labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833a447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.fc(x)\n",
    "        return x    # latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ef22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, out_channels,\n",
    "            kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.sigmoid(x)    # nn.BCELoss()\n",
    "        return x    # recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83797a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.decoder = decoder.to(self.device)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, images):\n",
    "        latent = self.encoder(images)\n",
    "        recon = self.decoder(latent)\n",
    "        return recon, latent\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        recon, latent = self.forward(images)\n",
    "        loss = self.loss_fn(recon, images)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return dict(loss=loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, images):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        recon, latent = self.forward(images)\n",
    "        loss = self.loss_fn(recon, images)\n",
    "        return dict(loss=loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pred_step(self, batch):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        labels = batch[\"label\"]\n",
    "        recon, latent = self.forward(images)\n",
    "        return dict(image=images, label=labels, latent=latent, recon=recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f20fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(latent_dim=2, in_channels=1)\n",
    "decoder = Decoder(latent_dim=2, out_channels=1)\n",
    "model = Autoencoder(encoder, decoder)\n",
    "\n",
    "# history = fit(model, train_loader, num_epochs=10, valid_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fef406",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_history = {}\n",
    "images = create_images(generator, z_sample)\n",
    "plot_images(*images, ncols=10, xunit=1, yunit=1)\n",
    "\n",
    "for _ in range(5):\n",
    "    history = fit(gan, train_loader, num_epochs=5)\n",
    "    for split_name, metrics in history.items():\n",
    "        total_history.setdefault(split_name, {})\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            total_history[split_name].setdefault(metric_name, [])\n",
    "            total_history[split_name][metric_name].extend(metric_values)\n",
    "\n",
    "    images = create_images(generator, z_sample)\n",
    "    plot_images(*images, ncols=10, xunit=1, yunit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
