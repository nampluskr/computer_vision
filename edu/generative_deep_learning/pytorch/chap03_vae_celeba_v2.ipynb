{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f96ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0f132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "BETA = 2000\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34919ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train dataset: 162770, dataloader: 1271\n",
      "train images: torch.Size([128, 3, 32, 32]), torch.float32, 0.0, 1.0\n",
      "train labels: torch.Size([128]), torch.int64, 0, 1\n",
      "\n",
      "valid dataset: 19867, dataloader: 621\n",
      "valid images: torch.Size([32, 3, 32, 32]), torch.float32, 0.0, 1.0\n",
      "valid labels: torch.Size([32]), torch.int64, 0, 1\n",
      "\n",
      "test dataset: 19962, dataloader: 624\n",
      "test  images: torch.Size([32, 3, 32, 32]), torch.float32, 0.0, 1.0\n",
      "test  labels: torch.Size([32]), torch.int64, 0, 1\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "from datasets import CelebA, get_train_loader, get_test_loader\n",
    "\n",
    "root_dir = \"/mnt/d/datasets/celeba\"\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=T.InterpolationMode.BILINEAR),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_loader = get_train_loader(dataset=CelebA(root_dir, \"train\", transform=transform), batch_size=BATCH_SIZE)\n",
    "valid_loader = get_test_loader(dataset=CelebA(root_dir, \"valid\", transform=transform), batch_size=32)\n",
    "test_loader = get_test_loader(dataset=CelebA(root_dir, \"test\", transform=transform), batch_size=32)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntrain dataset: {len(train_loader.dataset)}, dataloader: {len(train_loader)}\")\n",
    "print(f\"train images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"train labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")\n",
    "\n",
    "batch = next(iter(valid_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\nvalid dataset: {len(valid_loader.dataset)}, dataloader: {len(valid_loader)}\")\n",
    "print(f\"valid images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"valid labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(f\"\\ntest dataset: {len(test_loader.dataset)}, dataloader: {len(test_loader)}\")\n",
    "print(f\"test  images: {images.shape}, {images.dtype}, {images.min()}, {images.max()}\")\n",
    "print(f\"test  labels: {labels.shape}, {labels.dtype}, {labels.min()}, {labels.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ae28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=Z_DIM, in_channels=CHANNELS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.flatten_size = NUM_FEATURES * (IMAGE_SIZE // 16) * (IMAGE_SIZE // 16)\n",
    "        self.fc1 = nn.Linear(self.flatten_size, latent_dim)\n",
    "        self.fc2 = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(mu)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, self.flatten_size)\n",
    "        mu, logvar = self.fc1(x), self.fc2(x)\n",
    "        latent = self.reparameterize(mu, logvar)\n",
    "        return latent, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeaa42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=Z_DIM, out_channels=CHANNELS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten_size = NUM_FEATURES * (IMAGE_SIZE // 16) * (IMAGE_SIZE // 16)\n",
    "        self.fc = nn.Linear(latent_dim, self.flatten_size)\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.deconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.deconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.deconv5 = nn.ConvTranspose2d(NUM_FEATURES, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, NUM_FEATURES, IMAGE_SIZE // 16, IMAGE_SIZE // 16)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.deconv5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a7e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modeling: Variational Autoencoder\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beta=500):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        # self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.bce_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, images):\n",
    "        latent, mu, logvar = self.encoder(images)\n",
    "        recon = self.decoder(latent)\n",
    "        return recon, latent, mu, logvar\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def loss_fn(self, recon, images, mu, logvar):\n",
    "        bce_pixel = self.bce_loss(recon, images)\n",
    "        bce = bce_pixel.view(bce_pixel.size(0), -1).mean(dim=1)    # (B,)\n",
    "        bce = self.beta * bce.mean() \n",
    "\n",
    "        kld_sample = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)  # (B,)\n",
    "        kld = kld_sample.mean()\n",
    "\n",
    "        loss =  bce + kld\n",
    "        return loss, bce, kld\n",
    "\n",
    "    def train_step(self, batch, optimizer):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, latent, mu, logvar = self.forward(images)\n",
    "        loss, bce, kld = self.loss_fn(recon, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return dict(loss=loss, bce=bce, kld=kld)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, batch):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        recon, latent, mu, logvar = self.forward(images)\n",
    "        loss, bce, kld = self.loss_fn(recon, images, mu, logvar)\n",
    "        return dict(loss=loss, bce=bce, kld=kld)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pred_step(self, batch):\n",
    "        images = batch[\"image\"].to(self.device)\n",
    "        labels = batch[\"label\"]\n",
    "        recon, latent, mu, logvar = self.forward(images)\n",
    "        return dict(image=images, label=labels, latent=latent, recon=recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c77ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import fit, evaluate, predict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(latent_dim=Z_DIM, in_channels=CHANNELS).to(device)\n",
    "decoder = Decoder(latent_dim=Z_DIM, out_channels=CHANNELS).to(device)\n",
    "model = VAE(encoder, decoder, beta=2000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/10] loss:66.696, bce:51.519, kld:15.177 | (val) loss:56.171, bce:40.111, kld:16.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2/10] loss:55.667, bce:39.757, kld:15.910 | (val) loss:54.333, bce:38.125, kld:16.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  48%|####8     | 615/1271 [00:47<00:48, 13.39it/s, loss=54.534, bce=38.403, kld=16.131]"
     ]
    }
   ],
   "source": [
    "history = fit(model, train_loader, optimizer, num_epochs=10, valid_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf72cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_images\n",
    "\n",
    "predictions = predict(model, test_loader)\n",
    "images = predictions[\"image\"]\n",
    "labels = predictions[\"label\"]\n",
    "latent = predictions[\"latent\"]\n",
    "recon  = predictions[\"recon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(*images[:10], ncols=10, xunit=1, yunit=1)\n",
    "show_images(*recon[:10], ncols=10, xunit=1, yunit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
