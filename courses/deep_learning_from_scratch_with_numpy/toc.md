# Course: Deep Learning from Scratch with NumPy

## Part 1: Foundations of Neural Networks

### Chapter 1: Mathematical Preliminaries
- 1.1 Linear Algebra Essentials
- 1.2 Calculus for Deep Learning
- 1.3 Optimization Basics

### Chapter 2: Building Blocks of Neural Networks
- 2.1 Activation Functions
- 2.2 Loss Functions
- 2.3 Weight Initialization

### Chapter 3: Backpropagation Algorithm
- 3.1 Overview
- 3.2 Gradient Derivation
- 3.3 Softmax + Cross-Entropy Gradient
- 3.4 Matrix Dimension Analysis
- 3.5 Computational Graph Visualization

---

## Part 2: MLP for Different Tasks

### Chapter 4: Multi-Layer Perceptron (MLP)
- 4.1 Network Architecture
- 4.2 Forward Propagation
- 4.3 Backward Propagation

### Chapter 5: Regression using MLP
- 5.1 Problem Definition
- 5.2 Mean Squared Error Loss
- 5.3 California Housing Dataset
- 5.4 Implementation and Experiment

### Chapter 6: Binary Classification using MLP
- 6.1 Problem Definition
- 6.2 Binary Cross-Entropy Loss
- 6.3 MNIST Binary Dataset (0 vs 1)
- 6.4 Implementation and Experiment

### Chapter 7: Multiclass Classification using MLP
- 7.1 Problem Definition
- 7.2 Categorical Cross-Entropy Loss
- 7.3 MNIST Dataset (10 Classes)
- 7.4 Implementation and Experiment

### Chapter 8: Task Comparison Summary
- 8.1 Output Layer Comparison
- 8.2 Loss Function Comparison
- 8.3 Gradient Pattern Comparison

---

## Part 3: Evolving Code Architecture

### Chapter 9: From Basic to PyTorch-Style
- 9.1 Version 1 - Basic Implementation
- 9.2 Version 2 - DataLoader
- 9.3 Version 3 - Module Abstraction
- 9.4 Version 4 - Optimizer
- 9.5 Version 5 - ReLU and Adam
- 9.6 Version 6 - Trainer
- 9.7 Evolution Summary

---

## Appendix

### Appendix A: Complete Code Listings

### Appendix B: Dataset Preparation

### Appendix C: NumPy Quick Reference
