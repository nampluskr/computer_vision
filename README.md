# BaseTrainer ì„¤ê³„ ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ íŠ¹ì§•](#í•µì‹¬-íŠ¹ì§•)
3. [êµ¬ì¡° ë° ì•„í‚¤í…ì²˜](#êµ¬ì¡°-ë°-ì•„í‚¤í…ì²˜)
4. [ì„¤ê³„ ì›ì¹™](#ì„¤ê³„-ì›ì¹™)
5. [ì£¼ìš” ì»´í¬ë„ŒíŠ¸](#ì£¼ìš”-ì»´í¬ë„ŒíŠ¸)
6. [Hook ì‹œìŠ¤í…œ](#hook-ì‹œìŠ¤í…œ)
7. [í™•ì¥ ë°©ë²•](#í™•ì¥-ë°©ë²•)
8. [í–¥í›„ ê°œì„  ì‚¬í•­](#í–¥í›„-ê°œì„ -ì‚¬í•­)

---

## ê°œìš”

`BaseTrainer`ëŠ” PyTorch ê¸°ë°˜ì˜ ì¶”ìƒ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, Template Method íŒ¨í„´ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ í‘œì¤€í™”í•˜ê³  í™•ì¥ ê°€ëŠ¥í•˜ê²Œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ
- **ì¬ì‚¬ìš©ì„±**: ê³µí†µ í•™ìŠµ ë¡œì§ì„ í•œ ê³³ì— ì§‘ì¤‘
- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ëª¨ë¸/íƒœìŠ¤í¬ì— ì‰½ê²Œ ì ìš©
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª…í™•í•œ êµ¬ì¡°ì™€ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
- **ì—°ì† í•™ìŠµ**: Checkpointë¥¼ í†µí•œ í•™ìŠµ ì¬ê°œ ì§€ì›

---

## í•µì‹¬ íŠ¹ì§•

### 1. **Template Method íŒ¨í„´**
```python
class BaseTrainer(ABC):
    @abstractmethod
    def training_step(self, batch, batch_idx):
        """ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„ í•„ìˆ˜"""
        raise NotImplementedError
    
    @abstractmethod
    def validation_step(self, batch, batch_idx):
        """ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„ í•„ìˆ˜"""
        raise NotImplementedError
```

### 2. **Hook ì‹œìŠ¤í…œ**
í•™ìŠµ ê³¼ì •ì˜ ê° ë‹¨ê³„ì— ì»¤ìŠ¤í…€ ë¡œì§ ì‚½ì… ê°€ëŠ¥
```python
def on_train_epoch_start(self): pass
def on_train_batch_end(self, outputs, batch, batch_idx): pass
def on_validation_epoch_end(self, outputs): pass
# ... ì´ 8ê°œì˜ hook ë©”ì„œë“œ
```

### 3. **ì—°ì† í•™ìŠµ ì§€ì›**
```python
self.global_epoch = 0    # ëˆ„ì  ì´ epoch ìˆ˜
self.global_step = 0     # ëˆ„ì  ì´ step ìˆ˜
self.history = {}        # í•™ìŠµ ì´ë ¥ ëˆ„ì 
```

### 4. **ìœ ì—°í•œ êµ¬ì„±**
```python
def configure_optimizers(self):
    return {
        'optimizer': optimizer,
        'scheduler': scheduler  # Optional
    }

def configure_early_stoppers(self):
    return {
        'train': EarlyStopper(...),  # Optional
        'valid': EarlyStopper(...)   # Optional
    }
```

---

## êµ¬ì¡° ë° ì•„í‚¤í…ì²˜

### í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BaseTrainer (ABC)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # ìƒíƒœ ë³€ìˆ˜                          â”‚
â”‚ - model                             â”‚
â”‚ - optimizer, scheduler              â”‚
â”‚ - global_epoch, global_step         â”‚
â”‚ - history                           â”‚
â”‚ - best_model_state                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # ì¶”ìƒ ë©”ì„œë“œ (êµ¬í˜„ í•„ìˆ˜)            â”‚
â”‚ + training_step()                   â”‚
â”‚ + validation_step()                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # êµ¬ì„± ë©”ì„œë“œ (Optional)             â”‚
â”‚ + configure_optimizers()            â”‚
â”‚ + configure_early_stoppers()        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # Hook ë©”ì„œë“œ (Override ê°€ëŠ¥)        â”‚
â”‚ + on_train_epoch_start()            â”‚
â”‚ + on_train_batch_end()              â”‚
â”‚ + on_validation_epoch_end()         â”‚
â”‚ + ...                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # í•µì‹¬ ë©”ì„œë“œ                        â”‚
â”‚ + fit()                             â”‚
â”‚ - _train_epoch()                    â”‚
â”‚ - _validate_epoch()                 â”‚
â”‚ - _update_scheduler()               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ # ìœ í‹¸ë¦¬í‹°                           â”‚
â”‚ + save_checkpoint()                 â”‚
â”‚ + load_checkpoint()                 â”‚
â”‚ + save_model()                      â”‚
â”‚ + load_model()                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²
           â”‚ ìƒì†
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚
ClassificationTrainer  AnomalyTrainer
```

### í•™ìŠµ í”Œë¡œìš°
```
fit()
  â”‚
  â”œâ”€> on_fit_start()
  â”‚     â”œâ”€> _setup_optimizers()
  â”‚     â””â”€> _setup_early_stoppers()
  â”‚
  â”œâ”€> for epoch in range(1, num_epochs + 1):
  â”‚     â”‚
  â”‚     â”œâ”€> on_train_epoch_start()
  â”‚     â”‚     â””â”€> model.train()
  â”‚     â”‚
  â”‚     â”œâ”€> _train_epoch(train_loader)
  â”‚     â”‚     â”œâ”€> for batch in train_loader:
  â”‚     â”‚     â”‚     â”œâ”€> on_train_batch_start()
  â”‚     â”‚     â”‚     â”œâ”€> training_step()  â† êµ¬í˜„ í•„ìˆ˜
  â”‚     â”‚     â”‚     â”œâ”€> loss.backward()
  â”‚     â”‚     â”‚     â”œâ”€> optimizer.step()
  â”‚     â”‚     â”‚     â”œâ”€> global_step += 1
  â”‚     â”‚     â”‚     â””â”€> on_train_batch_end()
  â”‚     â”‚     â””â”€> return averaged outputs
  â”‚     â”‚
  â”‚     â”œâ”€> on_train_epoch_end()
  â”‚     â”‚     â””â”€> _update_history()
  â”‚     â”‚
  â”‚     â”œâ”€> _check_train_stopping()
  â”‚     â”‚
  â”‚     â”œâ”€> if has_valid_loader:
  â”‚     â”‚     â”œâ”€> on_validation_epoch_start()
  â”‚     â”‚     â”‚     â””â”€> model.eval()
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€> _validate_epoch(valid_loader)
  â”‚     â”‚     â”‚     â”œâ”€> for batch in valid_loader:
  â”‚     â”‚     â”‚     â”‚     â”œâ”€> on_validation_batch_start()
  â”‚     â”‚     â”‚     â”‚     â”œâ”€> validation_step()  â† êµ¬í˜„ í•„ìˆ˜
  â”‚     â”‚     â”‚     â”‚     â””â”€> on_validation_batch_end()
  â”‚     â”‚     â”‚     â””â”€> return averaged outputs
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€> on_validation_epoch_end()
  â”‚     â”‚     â”‚     â””â”€> _update_history()
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€> _check_valid_stopping()
  â”‚     â”‚           â””â”€> save best_model_state
  â”‚     â”‚
  â”‚     â””â”€> _update_scheduler()
  â”‚           â””â”€> log LR changes
  â”‚
  â””â”€> on_fit_end()
        â””â”€> save best model
```

---

## ì„¤ê³„ ì›ì¹™

### 1. **ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬ (Separation of Concerns)**
```python
# âœ… ì¢‹ì€ ì˜ˆ: ê° ë©”ì„œë“œê°€ ëª…í™•í•œ ì±…ì„
def _train_epoch(self, train_loader):
    """í•œ epochì˜ í•™ìŠµë§Œ ë‹´ë‹¹"""
    # ...

def _validate_epoch(self, valid_loader):
    """í•œ epochì˜ ê²€ì¦ë§Œ ë‹´ë‹¹"""
    # ...

def _update_scheduler(self):
    """Scheduler ì—…ë°ì´íŠ¸ë§Œ ë‹´ë‹¹"""
    # ...
```

### 2. **ê°œë°©-íì‡„ ì›ì¹™ (Open-Closed Principle)**
```python
# âœ… í™•ì¥ì—ëŠ” ì—´ë ¤ìˆê³ , ìˆ˜ì •ì—ëŠ” ë‹«í˜€ìˆìŒ
class ClassificationTrainer(BaseTrainer):
    def training_step(self, batch, batch_idx):
        """ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ (BaseTrainer ìˆ˜ì • ë¶ˆí•„ìš”)"""
        images = batch["image"].to(self.device)
        labels = batch["label"].to(self.device)
        
        logits = self.model(images)
        loss = self.loss_fn(logits, labels)
        acc = (logits.argmax(1) == labels).float().mean()
        
        return dict(loss=loss, acc=acc)
```

### 3. **ì˜ì¡´ì„± ì—­ì „ ì›ì¹™ (Dependency Inversion)**
```python
# âœ… ì¶”ìƒí™”ì— ì˜ì¡´
def __init__(self, model, loss_fn=None, device=None, logger=None):
    self.model = model              # êµ¬ì²´ì ì¸ ëª¨ë¸ì´ ì•„ë‹Œ nn.Module
    self.loss_fn = loss_fn          # êµ¬ì²´ì ì¸ lossê°€ ì•„ë‹Œ callable
    self.logger = logger            # êµ¬ì²´ì ì¸ loggerê°€ ì•„ë‹Œ interface
```

### 4. **ë‹¨ì¼ ì±…ì„ ì›ì¹™ (Single Responsibility)**
```python
# âœ… ê° ë©”ì„œë“œê°€ í•˜ë‚˜ì˜ ëª…í™•í•œ ì±…ì„
def _setup_optimizers(self):
    """Optimizer ì„¤ì •ë§Œ ë‹´ë‹¹"""

def _setup_early_stoppers(self):
    """Early Stopper ì„¤ì •ë§Œ ë‹´ë‹¹"""

def _format_time(self, seconds):
    """ì‹œê°„ í¬ë§·íŒ…ë§Œ ë‹´ë‹¹"""

def _update_scheduler(self):
    """Scheduler ì—…ë°ì´íŠ¸ë§Œ ë‹´ë‹¹"""
```

### 5. **Don't Repeat Yourself (DRY)**
```python
# âœ… ê³µí†µ ë¡œì§ì€ í•œ ê³³ì—
def _update_history(self, outputs):
    """Train/Valid ëª¨ë‘ì—ì„œ ì¬ì‚¬ìš©"""
    history_key = 'train' if self.training else 'valid'
    for key, value in outputs.items():
        self.history[history_key].setdefault(key, [])
        self.history[history_key][key].append(value)
```

### 6. **Fail Fast**
```python
# âœ… ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬
def __init__(self, model, ...):
    if model is None:
        raise ValueError("Model must be provided")
    
def _check_valid_stopping(self, valid_outputs):
    monitor_key = self.valid_early_stopper.monitor
    if monitor_key not in valid_outputs:
        self.logger.error(f"Monitor key '{monitor_key}' not found")
        return False
```

---

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. **ìƒíƒœ ë³€ìˆ˜**

#### **í•™ìŠµ ì§„í–‰ ìƒíƒœ**
```python
self.epoch           # í˜„ì¬ fitì˜ epoch (1~num_epochs)
self.global_epoch    # ëˆ„ì  ì´ epoch ìˆ˜ (0~âˆ)
self.global_step     # ëˆ„ì  ì´ step ìˆ˜ (0~âˆ)
self.training        # True: train mode, False: eval mode
```

#### **ëª¨ë¸ ê´€ë ¨**
```python
self.model              # í•™ìŠµí•  ëª¨ë¸
self.device             # í•™ìŠµ ë””ë°”ì´ìŠ¤ (cuda/cpu)
self.loss_fn            # ì†ì‹¤ í•¨ìˆ˜
self.best_model_state   # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ state_dict
```

#### **ìµœì í™” ê´€ë ¨**
```python
self.optimizer          # Optimizer (Optional)
self.scheduler          # LR Scheduler (Optional)
self.train_early_stopper  # Train Early Stopper (Optional)
self.valid_early_stopper  # Valid Early Stopper (Optional)
```

#### **ê¸°ë¡ ê´€ë ¨**
```python
self.history            # {'train': {...}, 'valid': {...}}
self.logger             # Logging ê°ì²´
self.output_dir         # ì¶œë ¥ ë””ë ‰í† ë¦¬
self.run_name           # ì‹¤í–‰ ì´ë¦„
```

### 2. **Early Stopper**
```python
class EarlyStopper:
    def __init__(self, patience=10, min_delta=1e-3, mode='max', 
                 target_value=None, monitor='loss'):
        self.patience = patience      # ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ epoch ìˆ˜
        self.min_delta = min_delta    # ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
        self.mode = mode              # 'max' or 'min'
        self.target_value = target_value  # ëª©í‘œê°’ (ë„ë‹¬í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ)
        self.monitor = monitor        # ëª¨ë‹ˆí„°ë§í•  metric ì´ë¦„
```

**íŠ¹ì§•:**
- Patience ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ
- Target value ë„ë‹¬ ì‹œ ì¢…ë£Œ
- Train/Valid ë³„ë„ ì„¤ì • ê°€ëŠ¥

### 3. **Logger ì‹œìŠ¤í…œ**
```python
def set_logger(output_dir, run_name):
    """ì½˜ì†” + íŒŒì¼ ë™ì‹œ ë¡œê¹…"""
    logger = logging.getLogger(run_name)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(message)s'))
    
    # File handler
    file_handler = logging.FileHandler(
        os.path.join(output_dir, f"{run_name}_training.log")
    )
    file_handler.setFormatter(
        logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    )
    
    return logger
```

### 4. **History ì‹œìŠ¤í…œ**
```python
self.history = {
    'train': {
        'loss': [0.5, 0.3, 0.2, ...],
        'acc': [0.8, 0.9, 0.95, ...]
    },
    'valid': {
        'loss': [0.6, 0.4, 0.3, ...],
        'acc': [0.75, 0.85, 0.92, ...]
    }
}
```

**íŠ¹ì§•:**
- Metricë³„ epoch ë‹¨ìœ„ ê¸°ë¡
- Checkpointì— ì €ì¥ë˜ì–´ ì—°ì† í•™ìŠµ ì‹œ ëˆ„ì 
- í•™ìŠµ í›„ ì‹œê°í™”/ë¶„ì„ ìš©ì´

---

## Hook ì‹œìŠ¤í…œ

### Hook ë©”ì„œë“œ ëª©ë¡

| Hook | í˜¸ì¶œ ì‹œì  | ìš©ë„ |
|------|----------|------|
| `on_fit_start()` | fit ì‹œì‘ ì‹œ | ì´ˆê¸°í™”, ë¡œê¹… |
| `on_fit_end()` | fit ì¢…ë£Œ ì‹œ | ìµœì¢… ì €ì¥, ìš”ì•½ |
| `on_train_epoch_start()` | Train epoch ì‹œì‘ | Model mode ì„¤ì • |
| `on_train_epoch_end(outputs)` | Train epoch ì¢…ë£Œ | History ì—…ë°ì´íŠ¸, ë¡œê¹… |
| `on_train_batch_start(batch, idx)` | Train batch ì‹œì‘ | Batch ì „ì²˜ë¦¬ |
| `on_train_batch_end(outputs, batch, idx)` | Train batch ì¢…ë£Œ | Gradient clipping, ë¡œê¹… |
| `on_validation_epoch_start()` | Valid epoch ì‹œì‘ | Model mode ì„¤ì • |
| `on_validation_epoch_end(outputs)` | Valid epoch ì¢…ë£Œ | History ì—…ë°ì´íŠ¸, ë¡œê¹… |
| `on_validation_batch_start(batch, idx)` | Valid batch ì‹œì‘ | Batch ì „ì²˜ë¦¬ |
| `on_validation_batch_end(outputs, batch, idx)` | Valid batch ì¢…ë£Œ | ì¤‘ê°„ ê²°ê³¼ ìˆ˜ì§‘ |

### Hook í™œìš© ì˜ˆì‹œ

#### **1. Gradient Clipping**
```python
def on_train_batch_end(self, outputs, batch, batch_idx):
    if self.optimizer is not None:
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
```

#### **2. ì£¼ê¸°ì ì¸ Checkpoint ì €ì¥**
```python
def on_train_batch_end(self, outputs, batch, batch_idx):
    if self.global_step % 1000 == 0 and self.global_step > 0:
        checkpoint_path = f"{self.output_dir}/checkpoint_step_{self.global_step}.pth"
        self.save_checkpoint(checkpoint_path)
```

#### **3. TensorBoard ë¡œê¹…**
```python
def on_train_batch_end(self, outputs, batch, batch_idx):
    self.writer.add_scalar('train/loss', outputs['loss'], self.global_step)
    self.writer.add_scalar('train/lr', 
                          self.optimizer.param_groups[0]['lr'], 
                          self.global_step)
```

#### **4. Learning Rate Warmup**
```python
def on_train_batch_end(self, outputs, batch, batch_idx):
    warmup_steps = 1000
    if self.global_step < warmup_steps:
        lr_scale = self.global_step / warmup_steps
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.base_lr * lr_scale
```

#### **5. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘**
```python
def on_validation_batch_end(self, outputs, batch, batch_idx):
    # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘
    if not hasattr(self, 'all_predictions'):
        self.all_predictions = []
        self.all_targets = []
    
    self.all_predictions.append(outputs['predictions'])
    self.all_targets.append(batch['label'])

def on_validation_epoch_end(self, outputs):
    # Epoch ëì— ì „ì²´ ê²°ê³¼ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°
    from sklearn.metrics import confusion_matrix
    
    predictions = torch.cat(self.all_predictions).cpu().numpy()
    targets = torch.cat(self.all_targets).cpu().numpy()
    
    cm = confusion_matrix(targets, predictions)
    self.logger.info(f"Confusion Matrix:\n{cm}")
    
    # ì´ˆê¸°í™”
    self.all_predictions = []
    self.all_targets = []
```

---

## í™•ì¥ ë°©ë²•

### 1. **Classification Trainer ì˜ˆì‹œ**
```python
class ClassificationTrainer(BaseTrainer):
    def __init__(self, model, num_classes, loss_fn=None, logger=None):
        super().__init__(model, loss_fn, logger=logger)
        self.num_classes = num_classes
    
    def training_step(self, batch, batch_idx):
        images = batch["image"].to(self.device)
        labels = batch["label"].to(self.device)
        
        logits = self.model(images)
        loss = self.loss_fn(logits, labels)
        acc = (logits.argmax(1) == labels).float().mean()
        
        return dict(loss=loss, acc=acc)
    
    def validation_step(self, batch, batch_idx):
        images = batch["image"].to(self.device)
        labels = batch["label"].to(self.device)
        
        logits = self.model(images)
        loss = self.loss_fn(logits, labels)
        acc = (logits.argmax(1) == labels).float().mean()
        
        return dict(loss=loss, acc=acc)
    
    def configure_optimizers(self):
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        return dict(optimizer=optimizer, scheduler=scheduler)
    
    def configure_early_stoppers(self):
        return dict(
            valid=EarlyStopper(patience=10, mode='max', monitor='acc', target_value=0.99)
        )
```

### 2. **Anomaly Detection Trainer ì˜ˆì‹œ**
```python
class AnomalyTrainer(BaseTrainer):
    def __init__(self, model, loss_fn=None, logger=None):
        super().__init__(model, loss_fn, logger=logger)
    
    def training_step(self, batch, batch_idx):
        images = batch["image"].to(self.device)
        
        # Autoencoder
        reconstructed = self.model(images)
        loss = self.loss_fn(reconstructed, images)
        
        return dict(loss=loss)
    
    def validation_step(self, batch, batch_idx):
        images = batch["image"].to(self.device)
        
        reconstructed = self.model(images)
        loss = self.loss_fn(reconstructed, images)
        
        return dict(loss=loss)
    
    def configure_optimizers(self):
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        return optimizer
    
    def configure_early_stoppers(self):
        return dict(
            valid=EarlyStopper(patience=15, mode='min', monitor='loss')
        )
```

---

## í–¥í›„ ê°œì„  ì‚¬í•­

### 1. â­ **Metric ê¸°ë°˜ Scheduler ì§€ì› (ReduceLROnPlateau)**

#### **í˜„ì¬ ë¬¸ì œ**
```python
# âŒ ReduceLROnPlateauëŠ” metric íŒŒë¼ë¯¸í„° í•„ìš”
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')
scheduler.step(valid_loss)  # metric í•„ìš”

# í˜„ì¬ëŠ” ë‹¨ìˆœíˆ scheduler.step()ë§Œ í˜¸ì¶œ
def _update_scheduler(self):
    self.scheduler.step()  # âŒ metric ì „ë‹¬ ì•ˆ ë¨
```

#### **ê°œì„  ë°©ì•ˆ**
```python
def _update_scheduler(self, valid_outputs=None):
    if self.optimizer is None or self.scheduler is None:
        return
    
    old_lrs = [group['lr'] for group in self.optimizer.param_groups]
    
    # ReduceLROnPlateau ê°ì§€
    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
        if valid_outputs is not None and hasattr(self, 'valid_early_stopper'):
            monitor_key = self.valid_early_stopper.monitor
            if monitor_key in valid_outputs:
                metric = valid_outputs[monitor_key]
                self.scheduler.step(metric)
            else:
                self.logger.warning(
                    f"ReduceLROnPlateau: monitor key '{monitor_key}' not found"
                )
        else:
            self.logger.warning("ReduceLROnPlateau requires validation outputs")
    else:
        # ì¼ë°˜ epoch ê¸°ë°˜ scheduler
        self.scheduler.step()
    
    new_lrs = [group['lr'] for group in self.optimizer.param_groups]
    
    # LR ë³€ê²½ ë¡œê¹…
    lr_threshold = 1e-8
    if len(new_lrs) == 1:
        if abs(old_lrs[0] - new_lrs[0]) > lr_threshold:
            self.logger.info(f"Learning rate updated: {old_lrs[0]:.6f} â†’ {new_lrs[0]:.6f}")
    else:
        for idx, (old_lr, new_lr) in enumerate(zip(old_lrs, new_lrs)):
            if abs(old_lr - new_lr) > lr_threshold:
                self.logger.info(f"Learning rate [group {idx}] updated: {old_lr:.6f} â†’ {new_lr:.6f}")

# fit ë©”ì„œë“œì—ì„œ í˜¸ì¶œ ìˆ˜ì •
def fit(self, ...):
    # ...
    if self.has_valid_loader:
        self.on_validation_epoch_start()
        valid_outputs = self._validate_epoch(valid_loader)
        self.on_validation_epoch_end(valid_outputs)
        if self._check_valid_stopping(valid_outputs):
            break
    
    self._update_scheduler(valid_outputs if self.has_valid_loader else None)
```

#### **ì‚¬ìš© ì˜ˆì‹œ**
```python
def configure_optimizers(self):
    optimizer = optim.Adam(self.model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=False
    )
    return dict(optimizer=optimizer, scheduler=scheduler)

def configure_early_stoppers(self):
    return dict(
        valid=EarlyStopper(patience=10, mode='min', monitor='loss')
    )

# ì¶œë ¥:
# [ 15/50] loss:0.023 | (val) loss:0.019 (2m 15s)
# Learning rate updated: 0.001000 â†’ 0.000500
```

---

### 2. â­ **Gradient Clipping**

#### **êµ¬í˜„ ë°©ì•ˆ 1: Hook í™œìš© (ê¶Œì¥)**
```python
class ClassificationTrainer(BaseTrainer):
    def __init__(self, model, num_classes, grad_clip_norm=None, ...):
        super().__init__(model, ...)
        self.grad_clip_norm = grad_clip_norm
    
    def on_train_batch_end(self, outputs, batch, batch_idx):
        if self.grad_clip_norm is not None and self.optimizer is not None:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                max_norm=self.grad_clip_norm
            )
```

#### **êµ¬í˜„ ë°©ì•ˆ 2: BaseTrainerì— í†µí•©**
```python
class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None, 
                 grad_clip_norm=None, grad_clip_value=None):
        # ...
        self.grad_clip_norm = grad_clip_norm
        self.grad_clip_value = grad_clip_value
    
    def _train_epoch(self, train_loader):
        # ...
        for batch_idx, batch in enumerate(progress_bar):
            # ...
            if self.optimizer is not None:
                self.optimizer.zero_grad()
                outputs = self.training_step(batch, batch_idx)
                loss = outputs.get('loss')
                
                if loss is not None:
                    loss.backward()
                    
                    # âœ… Gradient Clipping
                    if self.grad_clip_norm is not None:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(), 
                            max_norm=self.grad_clip_norm
                        )
                    if self.grad_clip_value is not None:
                        torch.nn.utils.clip_grad_value_(
                            self.model.parameters(), 
                            clip_value=self.grad_clip_value
                        )
                    
                    self.optimizer.step()
                self.global_step += 1
```

#### **ì‚¬ìš© ì˜ˆì‹œ**
```python
# ë°©ë²• 1: ìƒì„±ìì—ì„œ ì„¤ì •
trainer = ClassificationTrainer(
    model, 
    num_classes=10, 
    grad_clip_norm=1.0,  # âœ… Max gradient norm
    logger=logger
)

# ë°©ë²• 2: Hook ì˜¤ë²„ë¼ì´ë“œ
class MyTrainer(BaseTrainer):
    def on_train_batch_end(self, outputs, batch, batch_idx):
        # Gradient norm ê³„ì‚° ë° ë¡œê¹…
        total_norm = 0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        if total_norm > 10.0:
            self.logger.warning(f"Large gradient norm: {total_norm:.2f}")
        
        # Clipping
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
```

---

### 3. â­ **Mixed Precision Training (AMP)**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 use_amp=False):
        # ...
        self.use_amp = use_amp
        self.scaler = torch.cuda.amp.GradScaler() if use_amp else None
    
    def _train_epoch(self, train_loader):
        accumulated_outputs = {}
        total_images = 0
        
        with tqdm(train_loader, leave=False, ascii=True) as progress_bar:
            progress_bar.set_description(f"{self.epoch_info} Training")
            for batch_idx, batch in enumerate(progress_bar):
                batch_size = batch["image"].shape[0]
                total_images += batch_size
                
                self.on_train_batch_start(batch, batch_idx)
                
                if self.optimizer is not None:
                    self.optimizer.zero_grad()
                    
                    # âœ… Mixed Precision Context
                    with torch.cuda.amp.autocast(enabled=self.use_amp):
                        outputs = self.training_step(batch, batch_idx)
                        loss = outputs.get('loss')
                    
                    if loss is not None:
                        if self.use_amp:
                            # âœ… Scaled backward
                            self.scaler.scale(loss).backward()
                            
                            # âœ… Gradient clipping (optional)
                            if self.grad_clip_norm is not None:
                                self.scaler.unscale_(self.optimizer)
                                torch.nn.utils.clip_grad_norm_(
                                    self.model.parameters(), 
                                    max_norm=self.grad_clip_norm
                                )
                            
                            # âœ… Scaled optimizer step
                            self.scaler.step(self.optimizer)
                            self.scaler.update()
                        else:
                            # ì¼ë°˜ backward
                            loss.backward()
                            
                            if self.grad_clip_norm is not None:
                                torch.nn.utils.clip_grad_norm_(
                                    self.model.parameters(), 
                                    max_norm=self.grad_clip_norm
                                )
                            
                            self.optimizer.step()
                    
                    self.global_step += 1
                else:
                    with torch.cuda.amp.autocast(enabled=self.use_amp):
                        outputs = self.training_step(batch, batch_idx)
                
                # outputsì˜ tensorë¥¼ item()ìœ¼ë¡œ ë³€í™˜
                for name, value in outputs.items():
                    if isinstance(value, torch.Tensor):
                        value = value.item()
                    accumulated_outputs.setdefault(name, 0.0)
                    accumulated_outputs[name] += value * batch_size
                
                progress_bar.set_postfix({
                    name: f"{total_value / total_images:.3f}"
                    for name, total_value in accumulated_outputs.items()
                })
                self.on_train_batch_end(outputs, batch, batch_idx)
        
        return {name: value / total_images for name, value in accumulated_outputs.items()}
    
    @torch.no_grad()
    def _validate_epoch(self, valid_loader):
        accumulated_outputs = {}
        total_images = 0
        
        with tqdm(valid_loader, leave=False, ascii=True) as progress_bar:
            progress_bar.set_description(f"{self.epoch_info} Validation")
            for batch_idx, batch in enumerate(progress_bar):
                batch_size = batch["image"].shape[0]
                total_images += batch_size
                
                self.on_validation_batch_start(batch, batch_idx)
                
                # âœ… Validationë„ AMP ì‚¬ìš©
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    outputs = self.validation_step(batch, batch_idx)
                
                for name, value in outputs.items():
                    if isinstance(value, torch.Tensor):
                        value = value.item()
                    accumulated_outputs.setdefault(name, 0.0)
                    accumulated_outputs[name] += value * batch_size
                
                progress_bar.set_postfix({
                    name: f"{total_value / total_images:.3f}"
                    for name, total_value in accumulated_outputs.items()
                })
                self.on_validation_batch_end(outputs, batch, batch_idx)
        
        return {name: value / total_images for name, value in accumulated_outputs.items()}
    
    def save_checkpoint(self, filepath):
        output_dir = os.path.dirname(filepath)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "global_step": self.global_step,
            "global_epoch": self.global_epoch,
            "history": self.history,
        }
        if self.optimizer is not None:
            checkpoint["optimizer_state_dict"] = self.optimizer.state_dict()
        if self.scheduler is not None:
            checkpoint["scheduler_state_dict"] = self.scheduler.state_dict()
        if self.scaler is not None:
            checkpoint["scaler_state_dict"] = self.scaler.state_dict()  # âœ… Scaler ì €ì¥
        
        torch.save(checkpoint, filepath)
        self.logger.info(f"Checkpoint saved: {filepath}")
    
    def load_checkpoint(self, filepath, strict=True):
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"], strict=strict)
        self.global_step = checkpoint.get("global_step", 0)
        self.global_epoch = checkpoint.get("global_epoch", 0)
        self.history = checkpoint.get("history", {"train": {}, "valid": {}})
        
        if "optimizer_state_dict" in checkpoint and self.optimizer is not None:
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        if "scheduler_state_dict" in checkpoint and self.scheduler is not None:
            self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        if "scaler_state_dict" in checkpoint and self.scaler is not None:
            self.scaler.load_state_dict(checkpoint["scaler_state_dict"])  # âœ… Scaler ë¡œë“œ
        
        self.logger.info(f"Checkpoint loaded: {filepath}")
        self.logger.info(f"Resumed from global_epoch: {self.global_epoch}, global_step: {self.global_step}")
```

#### **ì‚¬ìš© ì˜ˆì‹œ**
```python
# âœ… Mixed Precision í™œì„±í™”
trainer = ClassificationTrainer(
    model,
    num_classes=10,
    loss_fn=loss_fn,
    logger=logger,
    use_amp=True  # âœ… AMP í™œì„±í™”
)

trainer.fit(train_loader, num_epochs=50, valid_loader=valid_loader)

# ì¥ì :
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~50% ê°ì†Œ
# 2. í•™ìŠµ ì†ë„ ~2x í–¥ìƒ (GPU dependent)
# 3. ì •í™•ë„ëŠ” ê±°ì˜ ë™ì¼
```

---

### 4. â­ **Gradient Accumulation**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 accumulation_steps=1):
        # ...
        self.accumulation_steps = accumulation_steps
    
    def _train_epoch(self, train_loader):
        accumulated_outputs = {}
        total_images = 0
        
        with tqdm(train_loader, leave=False, ascii=True) as progress_bar:
            progress_bar.set_description(f"{self.epoch_info} Training")
            for batch_idx, batch in enumerate(progress_bar):
                batch_size = batch["image"].shape[0]
                total_images += batch_size
                
                self.on_train_batch_start(batch, batch_idx)
                
                if self.optimizer is not None:
                    # âœ… accumulation_steps ë²ˆì§¸ batchë§ˆë‹¤ë§Œ zero_grad
                    if batch_idx % self.accumulation_steps == 0:
                        self.optimizer.zero_grad()
                    
                    outputs = self.training_step(batch, batch_idx)
                    loss = outputs.get('loss')
                    
                    if loss is not None:
                        # âœ… Loss scaling
                        scaled_loss = loss / self.accumulation_steps
                        scaled_loss.backward()
                        
                        # âœ… accumulation_steps ë²ˆì§¸ batchë§ˆë‹¤ë§Œ optimizer step
                        if (batch_idx + 1) % self.accumulation_steps == 0:
                            if self.grad_clip_norm is not None:
                                torch.nn.utils.clip_grad_norm_(
                                    self.model.parameters(), 
                                    max_norm=self.grad_clip_norm
                                )
                            self.optimizer.step()
                            self.global_step += 1
                else:
                    outputs = self.training_step(batch, batch_idx)
                
                # ...
```

#### **ì‚¬ìš© ì˜ˆì‹œ**
```python
# âœ… Effective batch size = 32 * 4 = 128
trainer = ClassificationTrainer(
    model,
    num_classes=10,
    loss_fn=loss_fn,
    logger=logger,
    accumulation_steps=4  # âœ… 4 batchë§ˆë‹¤ update
)

train_loader = DataLoader(dataset, batch_size=32)  # ì‹¤ì œ batch sizeëŠ” 32
trainer.fit(train_loader, num_epochs=50)

# íš¨ê³¼:
# - ë©”ëª¨ë¦¬ëŠ” batch_size=32ë§Œ ì‚¬ìš©
# - í•™ìŠµ íš¨ê³¼ëŠ” batch_size=128ê³¼ ë™ì¼
```

---

### 5. â­ **Distributed Training (DDP)**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 distributed=False, local_rank=0):
        self.distributed = distributed
        self.local_rank = local_rank
        
        if distributed:
            self.device = torch.device(f"cuda:{local_rank}")
            model = model.to(self.device)
            self.model = DDP(model, device_ids=[local_rank])
        else:
            self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model = model.to(self.device)
        
        # ...
    
    def _train_epoch(self, train_loader):
        # DDPì—ì„œëŠ” samplerì˜ epoch ì„¤ì • í•„ìš”
        if self.distributed and hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(self.global_epoch)
        
        # ...
    
    def save_checkpoint(self, filepath):
        # DDPì—ì„œëŠ” rank 0ë§Œ ì €ì¥
        if self.distributed and self.local_rank != 0:
            return
        
        output_dir = os.path.dirname(filepath)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # DDPì˜ ê²½ìš° model.module.state_dict() ì‚¬ìš©
        model_state = self.model.module.state_dict() if self.distributed else self.model.state_dict()
        
        checkpoint = {
            "model_state_dict": model_state,
            "global_step": self.global_step,
            "global_epoch": self.global_epoch,
            "history": self.history,
        }
        # ...
```

#### **ì‚¬ìš© ì˜ˆì‹œ**
```python
# launch script: torchrun --nproc_per_node=4 train.py

import torch.distributed as dist

def main():
    # DDP ì´ˆê¸°í™”
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ['LOCAL_RANK'])
    
    # DistributedSampler ì‚¬ìš©
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=dist.get_world_size(),
        rank=local_rank
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,
        sampler=train_sampler,  # âœ… Shuffle ëŒ€ì‹  sampler
        num_workers=4,
        pin_memory=True
    )
    
    # Trainer ìƒì„±
    trainer = ClassificationTrainer(
        model,
        num_classes=10,
        loss_fn=loss_fn,
        logger=logger,
        distributed=True,
        local_rank=local_rank
    )
    
    trainer.fit(train_loader, num_epochs=50, valid_loader=valid_loader)
    
    dist.destroy_process_group()

if __name__ == '__main__':
    main()
```

---

### 6. â­ **EMA (Exponential Moving Average)**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class EMA:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()
    
    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}

class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 use_ema=False, ema_decay=0.999):
        # ...
        self.use_ema = use_ema
        self.ema = EMA(self.model, decay=ema_decay) if use_ema else None
    
    def on_train_batch_end(self, outputs, batch, batch_idx):
        if self.ema is not None:
            self.ema.update()
    
    def _validate_epoch(self, valid_loader):
        # Validation ì‹œ EMA ëª¨ë¸ ì‚¬ìš©
        if self.ema is not None:
            self.ema.apply_shadow()
        
        accumulated_outputs = {}
        # ... validation logic
        
        if self.ema is not None:
            self.ema.restore()
        
        return accumulated_outputs
```

---

### 7. â­ **Progress Callback & Metrics Tracking**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class Callback:
    def on_fit_start(self, trainer): pass
    def on_fit_end(self, trainer): pass
    def on_epoch_start(self, trainer): pass
    def on_epoch_end(self, trainer, outputs): pass
    def on_batch_end(self, trainer, outputs): pass

class TensorBoardCallback(Callback):
    def __init__(self, log_dir):
        from torch.utils.tensorboard import SummaryWriter
        self.writer = SummaryWriter(log_dir)
    
    def on_batch_end(self, trainer, outputs):
        if trainer.training:
            for key, value in outputs.items():
                self.writer.add_scalar(f'train/{key}', value, trainer.global_step)
            self.writer.add_scalar('train/lr', 
                                 trainer.optimizer.param_groups[0]['lr'], 
                                 trainer.global_step)
    
    def on_epoch_end(self, trainer, outputs):
        for key, value in outputs.items():
            phase = 'train' if trainer.training else 'valid'
            self.writer.add_scalar(f'{phase}/{key}', value, trainer.global_epoch)

class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 callbacks=None):
        # ...
        self.callbacks = callbacks or []
    
    def on_fit_start(self):
        # ...
        for callback in self.callbacks:
            callback.on_fit_start(self)
    
    def on_train_batch_end(self, outputs, batch, batch_idx):
        for callback in self.callbacks:
            callback.on_batch_end(self, outputs)

# ì‚¬ìš© ì˜ˆì‹œ
trainer = ClassificationTrainer(
    model,
    callbacks=[
        TensorBoardCallback('./runs/experiment1'),
        WandBCallback(project='my-project'),
        CheckpointCallback(save_every=5)
    ]
)
```

---

### 8. â­ **Model Profiling & Debugging**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 profile=False):
        # ...
        self.profile = profile
    
    def _train_epoch(self, train_loader):
        if self.profile:
            from torch.profiler import profile, ProfilerActivity
            
            with profile(
                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                record_shapes=True,
                with_stack=True
            ) as prof:
                # ì²« ëª‡ batchë§Œ í”„ë¡œíŒŒì¼ë§
                for batch_idx, batch in enumerate(train_loader):
                    if batch_idx >= 10:
                        break
                    # ... training logic
            
            # í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥
            prof.export_chrome_trace(f"{self.output_dir}/profile_trace.json")
            self.logger.info(f"Profile saved to {self.output_dir}/profile_trace.json")
        
        # ì¼ë°˜ í•™ìŠµ ì§„í–‰
        accumulated_outputs = {}
        # ...
```

---

### 9. â­ **Auto Resume from Last Checkpoint**

#### **êµ¬í˜„ ë°©ì•ˆ**
```python
class BaseTrainer(ABC):
    def fit(self, train_loader, num_epochs, valid_loader=None, 
            output_dir=None, run_name=None, auto_resume=True):
        self.has_valid_loader = valid_loader is not None
        self.num_epochs = num_epochs
        self.output_dir = output_dir
        self.run_name = run_name or "best_model"
        
        # âœ… Auto resume
        if auto_resume and output_dir is not None:
            last_checkpoint = self._find_last_checkpoint()
            if last_checkpoint is not None:
                self.logger.info(f"Found checkpoint: {last_checkpoint}")
                self.load_checkpoint(last_checkpoint)
        
        self.on_fit_start()
        # ...
    
    def _find_last_checkpoint(self):
        """ë§ˆì§€ë§‰ checkpoint ì°¾ê¸°"""
        if not os.path.exists(self.output_dir):
            return None
        
        checkpoints = [
            f for f in os.listdir(self.output_dir)
            if f.startswith('checkpoint_') and f.endswith('.pth')
        ]
        
        if not checkpoints:
            return None
        
        # Epoch ë²ˆí˜¸ë¡œ ì •ë ¬
        checkpoints.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))
        return os.path.join(self.output_dir, checkpoints[-1])
```

---

### 10. â­ **Configurable Batch Dictionary Keys**

#### **í˜„ì¬ ë¬¸ì œ**
```python
# âŒ batch["image"]ì™€ batch["label"]ì´ í•˜ë“œì½”ë”©ë¨
batch_size = batch["image"].shape[0]
```

#### **ê°œì„  ë°©ì•ˆ**
```python
class BaseTrainer(ABC):
    def __init__(self, model, loss_fn=None, device=None, logger=None,
                 batch_image_key='image', batch_size_dim=0):
        # ...
        self.batch_image_key = batch_image_key
        self.batch_size_dim = batch_size_dim
    
    def _train_epoch(self, train_loader):
        accumulated_outputs = {}
        total_images = 0
        
        with tqdm(train_loader, leave=False, ascii=True) as progress_bar:
            for batch_idx, batch in enumerate(progress_bar):
                # âœ… Configurable key
                batch_size = batch[self.batch_image_key].shape[self.batch_size_dim]
                total_images += batch_size
                # ...

# ì‚¬ìš© ì˜ˆì‹œ
trainer = ClassificationTrainer(
    model,
    batch_image_key='input',  # âœ… 'image' ëŒ€ì‹  'input' ì‚¬ìš©
    batch_size_dim=0
)
```

---

## ğŸ“Š ê°œì„  ìš°ì„ ìˆœìœ„

| ìˆœìœ„ | í•­ëª© | ì¤‘ìš”ë„ | ë‚œì´ë„ | ì˜í–¥ ë²”ìœ„ |
|------|------|--------|--------|----------|
| 1 | Gradient Clipping | â­â­â­ | ë‚®ìŒ | í•™ìŠµ ì•ˆì •ì„± |
| 2 | Mixed Precision (AMP) | â­â­â­ | ì¤‘ê°„ | ë©”ëª¨ë¦¬, ì†ë„ |
| 3 | Metric ê¸°ë°˜ Scheduler | â­â­â­ | ë‚®ìŒ | LR ì¡°ì • |
| 4 | Gradient Accumulation | â­â­ | ì¤‘ê°„ | ë©”ëª¨ë¦¬ |
| 5 | Callback System | â­â­ | ì¤‘ê°„ | í™•ì¥ì„± |
| 6 | Auto Resume | â­â­ | ë‚®ìŒ | í¸ì˜ì„± |
| 7 | EMA | â­â­ | ì¤‘ê°„ | ì„±ëŠ¥ |
| 8 | Distributed Training | â­ | ë†’ìŒ | ì†ë„ |
| 9 | Model Profiling | â­ | ë‚®ìŒ | ë””ë²„ê¹… |
| 10 | Configurable Keys | â­ | ë‚®ìŒ | ìœ ì—°ì„± |

---

## âœ… ìš”ì•½

### **í˜„ì¬ BaseTrainerì˜ ê°•ì **
1. âœ… ëª…í™•í•œ ì¶”ìƒí™”ì™€ í™•ì¥ ì¸í„°í˜ì´ìŠ¤
2. âœ… Hook ì‹œìŠ¤í…œìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ìš©ì´
3. âœ… ì—°ì† í•™ìŠµ ì§€ì› (checkpoint, history)
4. âœ… Early stopping ì§€ì›
5. âœ… Scheduler í†µí•© ë° LR ë³€ê²½ ë¡œê¹…
6. âœ… ê¹”ë”í•œ ë¡œê¹… ì‹œìŠ¤í…œ

### **í•µì‹¬ ê°œì„  í•„ìš” ì‚¬í•­**
1. ğŸ”§ Metric ê¸°ë°˜ Scheduler (ReduceLROnPlateau)
2. ğŸ”§ Gradient Clipping
3. ğŸ”§ Mixed Precision Training
4. ğŸ”§ Gradient Accumulation
5. ğŸ”§ Callback System

ì´ëŸ¬í•œ ê°œì„ ì„ í†µí•´ BaseTrainerëŠ” ë”ìš± ê°•ë ¥í•˜ê³  ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ê°€ ë  ê²ƒì…ë‹ˆë‹¤! ğŸ‰